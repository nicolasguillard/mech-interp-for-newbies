# Feature

>[!quote] from "[Mechanistic Interpretability for AI Safety A Review](https://leonardbereska.github.io/blog/2024/mechinterpreview/)" [[References#^7739fa|(Bereska and al. - 2024)]]
>Features are the fundamental units of neural network [[Representation of Features|representations]] that cannot be further [[Decomposability|decomposed]] into simpler independent factors.
>
>_Alternative_ : Features are elements that a network would ideally assign to individual [[Neuron|neurons]] if neuron count were not a limiting factor (Bricken et al., 2023). In other words, features correspond to the disentangled [[Concept|concepts]] that a larger, sparser network with sufficient capacity would learn to [[Representation of Features|represent]] with individual neurons.

>[!quote] from "[A Comprehensive Mechanistic Interpretability Explainer & Glossary](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=eL6tFQqNwd4LbYlO1DVIen8K)" [[References#^d1d4d1|(Neel Nanda - 12/2022)]]
>A feature is a property of an input to the model, or some subset of that input (eg a token in the prompt given to a language model, or a patch of an image).