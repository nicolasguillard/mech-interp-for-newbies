# Superposition

>[!quote] from "[Mechanistic Interpretability for AI Safety A Review](https://leonardbereska.github.io/blog/2024/mechinterpreview/)" [[References#^7739fa|(Bereska and al. - 2024)]]
>Neural networks represent more [[Feature|features]] than they have [[Neuron|neurons]] by encoding features in overlapping combinations of neurons.

>[!quote] from "[A Comprehensive Mechanistic Interpretability Explainer & Glossary](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=eL6tFQqNwd4LbYlO1DVIen8K)" [[References#^d1d4d1|(Neel Nanda - 12/2022)]]
>Superposition is when a model represents more than n [[Feature|features]] in an n dimensional activation space. That is, features still correspond to directions, but the set of interpretable directions is larger than the number of dimensions. Intuitively, this is the model *simulating a larger model*.